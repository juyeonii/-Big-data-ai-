{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfcd9af-c13a-40c7-871b-7fc7c7edca68",
   "metadata": {
    "tags": []
   },
   "source": [
    "# < Object Detection - YOLOv5>\n",
    "- [yolov5 github](https://github.com/ultralytics/yolov5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01c3c19-84a1-44a5-8d8f-551f690a4783",
   "metadata": {},
   "source": [
    "               ![YOLOv5_types](./data/jupyter_figure/yolov5_types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b1560-1594-4b16-9925-87498c67493e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0. Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23bae82-a2a9-43d7-b907-8b02e39579bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tesla V100-PCIE-16GB'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch #import pytorch packages\n",
    "torch.cuda.is_available() # Check GPU is available\n",
    "torch.cuda.get_device_name(0) # Check GPU Device Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ece2f-a580-4c4e-8612-7179dc57c349",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Image Inference : Use detect.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1a458-71b0-45dd-844d-780f94aa6b19",
   "metadata": {},
   "source": [
    "- YOLOv5에서 기본적으로 제공해주는 detect.py를 사용한 객체 탐지\n",
    "- detect.py는 coco 데이터셋으로 미리 학습된 yolov5s weights를 기본으로 사용함\n",
    "- weights를 변경하고 싶을 경우(ex. 내가 학습시킨 모델) detect.py 실행 시 argument로 --weight 모델경로/모델파일이름.pt 를 넘겨주면 됨\n",
    "- 객체 검출 결과는 yolov5/runs/detect/ 에 저장됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1765a8b8-5e02-442c-87d6-33077fd19316",
   "metadata": {},
   "source": [
    "### ※ COCO dataset & Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5945bd-a2b8-480f-99fe-89ba746b3034",
   "metadata": {},
   "source": [
    "- COCO 데이터셋은 대규모 Object Detection / Segmentation을 위한 데이터세트\n",
    "- Object Detection 및 Segmentation 모델 학습 및 성능평가에 주로 사용되는 데이터셋\n",
    "- 2014/2015/2017년 데이터셋이 존재하며 매년 이미지와 클래스 수가 증가하였음. 우리가 실습에 사용할 데이터셋은 2017 Validation Set\n",
    "- Train Dataset : http://images.cocodataset.org/zips/train2017.zip\n",
    "- Validation Dataset : http://images.cocodataset.org/zips/val2017.zip\n",
    "- Test Dataset : http://images.cocodataset.org/zips/test2017.zip\n",
    "- Annotation File : http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e893726f-d3fb-4bcb-98b0-a5349702b78f",
   "metadata": {},
   "source": [
    "### ※ YOLO format COCO dataset Annotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ae906d-cc2b-4b68-b591-f261d15c6299",
   "metadata": {},
   "source": [
    "- 일반적인 Object Detection의 데이터 어노테이션 파일은 json 형태의 파일 포멧을 사용함\n",
    "- 하지만 YOLO 는 파일명.txt 형태의 텍스트 파일에 객체 클래스와 좌표를 적는 식의 독자적인 포멧을 사용함\n",
    "- 따라서 YOLO format으로 변환된 coco dataset 어노테이션 파일이 필요하며, 이는 아래 링크에서 다운받을 수 있음\n",
    "- Train / Validation Annotation File: https://ultralytics.com/assets/coco2017labels.zip\n",
    "- Validation Image Dataset & Annotation File : https://ultralytics.com/assets/coco2017val.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8764462-0164-4f96-b046-a2dc5593f5e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### ※ detect.py Argument 목록 예시"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cf3a26-2e08-43d8-b1fd-a47e445b9c4b",
   "metadata": {},
   "source": [
    "``` shell\n",
    "python detect.py --source 0  # webcam  \n",
    "　　　　　　　　　　　　　img.jpg  # image  \n",
    "　　　　　　　　　　　　　vid.mp4  # video  \n",
    "　　　　　　　　　　　　　path/  # directory  \n",
    "　　　　　　　　　　　　　path/*.jpg  # glob  \n",
    "　　　　　　　　　　　　　'https://youtu.be/Zgi9g1ksQHc'  # YouTube  \n",
    "　　　　　　　　　　　　　'rtsp://example.com/media.mp4'  # RTSP, RTMP, HTTP stream\n",
    "```\n",
    "             \n",
    "- 더 자세한 Argument는 detect.py를 열어보면 확인할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c460bc4b-b714-4f81-842c-9c11a6037c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd yolov5 && python detect.py --source ../data/coco_val/images/val2017/000000404922.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1dc25-aa3c-43d2-894a-4523a050b443",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Video Inference : Use detect.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40f0cdc-b985-4fa5-94c1-bd53d74976f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd yolov5 && python detect.py --source ../data/video_data/input_video.mp4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac95e6-581c-4c2a-8f3d-b12e1c892fce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Inference Image using pytorch load model function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9989db-87e8-4650-a014-620b505222e0",
   "metadata": {},
   "source": [
    "- YOLOv5 모델을 파이토치에서 직접 불러와 사용하는 방식.\n",
    "- 입력과 출력에 대한 작업을 자유롭게 조정할 수 있다는 장점이 있다.\n",
    "- 또한 객체 검출 뿐만 아니라 다른 부가적인 기능들을 구현해야 할때 사용이 용이하다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99766dae-3490-4af6-aa90-20240d346d22",
   "metadata": {},
   "source": [
    "## 1) Load COCO Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0e53d1-4ca5-44bf-9fee-9cbfe5c5928b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (1) Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff22b7ad-3116-443e-9c1a-67b6dedf7eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618c97e-2cfa-4491-b4b7-6dc0c688b648",
   "metadata": {},
   "source": [
    "### (2) Load Image from COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c358f763-4b08-49a6-8855-71d1828050a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/coco_val/images/val2017/'\n",
    "data_list = os.listdir(data_path)\n",
    "data_name = '000000404922.jpg'\n",
    "\n",
    "org_img = cv2.imread(data_path + data_name,cv2.IMREAD_COLOR)\n",
    "org_img = cv2.cvtColor(org_img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "result_img = cv2.imread(data_path + data_name,cv2.IMREAD_COLOR)\n",
    "result_img = cv2.cvtColor(result_img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "gt_img = cv2.imread(data_path + data_name,cv2.IMREAD_COLOR)\n",
    "gt_img = cv2.cvtColor(gt_img,cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b93ae84-6475-4a50-bf83-965f2c8415f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (3) Plot loaded Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a77778-427c-4004-af76-9ed3093d8078",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(org_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd086e6c-2a2f-43ff-a741-ec7577e4e6c0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2) Object Detect by YOLO V5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465c7821-0a43-48ce-99d1-3064257e3b07",
   "metadata": {},
   "source": [
    "### (1) Load Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a6b565-217d-4178-a50d-70a826a5a040",
   "metadata": {},
   "source": [
    "               ![YOLO v5](./data/jupyter_figure/YOLOv5_Architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507c445-462f-43b4-8657-f5cb79a7bbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load('ultralytics/yolov5','yolov5s') # or yolov5m, yolov5l, yolov5x, custom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d14e960-6b84-4a2d-9ef4-99d0afd6679d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (2)  Localization & Predict Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15f619d-54d7-4fac-9e9f-9614df9d36ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict image 0\n",
    "results = model(result_img)\n",
    "df = results.pandas().xyxy[0]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cff3de-832d-45fd-83e6-2c3bf064c479",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = []\n",
    "for i in range(len(df)) :\n",
    "    obj_xmin = int(df['xmin'][i])\n",
    "    obj_ymin = int(df['ymin'][i])\n",
    "    obj_xmax = int(df['xmax'][i])\n",
    "    obj_ymax = int(df['ymax'][i])\n",
    "    obj_confi = df['confidence'][i]\n",
    "    obj_class = df['class'][i]\n",
    "    obj_name = df['name'][i]\n",
    "    obj_dict = {\n",
    "                'class' : obj_name, \n",
    "                'confidence' : obj_confi, \n",
    "                'bbox' : [obj_xmin,obj_ymin,obj_xmax,obj_ymax]\n",
    "    }\n",
    "    obj_list.append(obj_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5932b568-ddff-4b51-83c1-5a08ff841500",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for obj_dict in obj_list :\n",
    "    print('[%d Object] \\n - class name : %s \\n - xmin : %d \\n - xmax : %d \\n - ymin : %d \\n - ymax : %d \\n - confidence : %f\\n'%(cnt, obj_dict['class'], obj_dict['bbox'][0], obj_dict['bbox'][1], obj_dict['bbox'][2], obj_dict['bbox'][3], obj_dict['confidence']))\n",
    "    cnt = cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4aea00-1025-4733-8b92-cf918793b037",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (3) Draw Rectangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a157a00-904a-4897-82a6-55e2d1e7f2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_color = {'person' : (255,0,0), #Red\n",
    "              'tennis racket' : (0,0,255), #Blue\n",
    "              'car' : (0,255,0) #Green\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10c48ee-06ff-4582-8536-91bbae0ab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj_dict in obj_list : \n",
    "    #TO DO - thickness = 3\n",
    "    \n",
    "    \n",
    "    cnt = cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16be7a5-c8e0-4a17-abdc-44f5b71ae9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(result_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73446031-ad51-4b80-ac10-29ec1602ffa3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### (4) Write Class name & Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e0d538-6771-420c-bc72-5032584a7a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj_dict in obj_list :\n",
    "    class_name = obj_dict['class']\n",
    "    confidence = \"%.2f\"%(obj_dict['confidence'])\n",
    "    text_x = obj_dict['bbox'][0] + 10 #xmin + 10\n",
    "    text_y = obj_dict['bbox'][1] + 20 #ymin + 10\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    #TO DO - fontscale = 0.7, thickness = 2\n",
    "    \n",
    "    \n",
    "    \n",
    "plt.figure(figsize = (15,8))\n",
    "plt.imshow(result_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8814e70-9210-46da-b75f-f29b74195c0b",
   "metadata": {},
   "source": [
    "### (5) Compare result with Ground Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580c9b6f-3b48-456b-a7ec-24edb5a5de3d",
   "metadata": {},
   "source": [
    "- Load COCO Dataset Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34497be7-21c4-48f2-9ad1-1f687a31184f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data/coco_val/annotations/instances_val2017.json\",\"r\") as json_file :\n",
    "    annotation = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd762c1-18f5-4d39-b459-83f0e1b58ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_id = data_name[6:].replace(\".jpg\",\"\")\n",
    "data_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6359b4ea-0184-470e-8259-dbfed3f2a0b4",
   "metadata": {},
   "source": [
    "- Load Groud Truth Information from Annotation File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a58cc4-5feb-48a5-bb2a-25666700c537",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_list = []\n",
    "obj_dict = {}\n",
    "print()\n",
    "for anno_data in annotation['annotations'] :\n",
    "    if anno_data['image_id'] == int(data_id) :\n",
    "        obj_dict['image_id'] = anno_data['image_id']\n",
    "        obj_dict['bbox'] = anno_data['bbox']\n",
    "        obj_dict['category_id'] = anno_data['category_id']\n",
    "        for categories in annotation['categories'] :\n",
    "            if obj_dict['category_id'] == categories['id'] : \n",
    "                obj_dict['class'] = categories['name']\n",
    "        anno_list.append(obj_dict)\n",
    "        obj_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d65382-adb1-418f-8538-08fcbee3f593",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f84436-7d96-429e-9348-6c452ef24968",
   "metadata": {},
   "source": [
    "- Change Coordinate (x,y,w,h) =====> (xmin,ymin,xmax,ymax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f01314-88a3-454b-bdc0-e997303eca80",
   "metadata": {},
   "source": [
    "                         ![Coordinate Examples](./data/jupyter_figure/coordinate_example.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b790c7bb-f772-45d0-ae9b-4f81fd17d2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno_dict in anno_list :\n",
    "    x = anno_dict['bbox'][0]\n",
    "    y = anno_dict['bbox'][1]\n",
    "    w = anno_dict['bbox'][2]\n",
    "    h = anno_dict['bbox'][3]\n",
    "    xmin = x\n",
    "    ymin = y\n",
    "    xmax = x + w\n",
    "    ymax = y + h\n",
    "    change_coord = list(map(int,[xmin,ymin,xmax,ymax]))\n",
    "    anno_dict['bbox'] = change_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab300804-ac12-49fd-938f-74a155cba028",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e364a1-1fae-4bd6-b0d8-5549760612ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_color = {'person' : (255,105,180), # hotpink\n",
    "            'tennis racket' : (135,206,235), # skyblue\n",
    "            #'bench' : (192,192,192), #Silver\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e176945-4d8a-4bd9-8591-65bac703d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno_dict in anno_list : \n",
    "    cv2.rectangle(gt_img, (anno_dict['bbox'][2],anno_dict['bbox'][3]), (anno_dict['bbox'][0],anno_dict['bbox'][1]), color=gt_color[anno_dict['class']],thickness=3)\n",
    "    cnt = cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c9f8e-2a5a-4a0e-85fc-1f7ef544ecbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(gt_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c838ff32-f26a-42cc-a736-7210eb516aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for anno_dict in anno_list :\n",
    "    class_name = anno_dict['class']\n",
    "    text_x = anno_dict['bbox'][0] + 10\n",
    "    text_y = anno_dict['bbox'][1] + 20\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(gt_img,class_name,(text_x,text_y),font,0.7,gt_color[class_name],2)\n",
    "    \n",
    "plt.figure(figsize = (10,6))\n",
    "plt.imshow(gt_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d716d2-7f64-4b33-87eb-4d7ab292cd02",
   "metadata": {},
   "source": [
    "- Compare Groud Truth & Prediction Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121c63a-7691-48fa-961d-1734e3cc056b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_list = [org_img,result_img,gt_img]\n",
    "fig = plt.figure(figsize=(25,8))\n",
    "rows = 1\n",
    "cols = 3\n",
    "xlabels = ['Original Image', 'Result Image', 'Ground Truth Image']\n",
    "for i in range(len(img_list)):\n",
    "    ax = fig.add_subplot(rows, cols, i+1)\n",
    "    ax.imshow(img_list[i])\n",
    "    ax.set_xlabel(xlabels[i],fontsize = 15)\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d62fb3-49b7-42ee-87b8-740f30ac87a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Evaluate Object Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e6bba3f-23bc-4f41-bfa7-a2c82f6be995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=/home/pirl/DH/pirl/2022청년AI교육자료/20기/01_OpenCV&Object_Detection/02_Object_Detection/yolov5/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.65, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=yolov5/runs/val, name=exp, exist_ok=False, half=True, dnn=False\n",
      "/bin/sh: 1: Object_Detection/02_Object_Detection/yolov5: not found\n",
      "fatal: cannot change to '/home/pirl/DH/pirl/2022청년AI교육자료/20기/01_OpenCV': 그런 파일이나 디렉터리가 없습니다\n",
      "YOLOv5 🚀 2022-6-20 Python-3.8.15 torch-1.10.2 CUDA:0 (Tesla V100-PCIE-16GB, 16161MiB)\n",
      "\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v6.1/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████████████████████████████████| 14.1M/14.1M [00:03<00:00, 3.99MB/s]\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning '/home/pirl/DH/pirl/2022청년AI교육자료/20기/01_OpenCV&Object_Detec\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/pirl/DH/pirl/2022청년AI교육자료/20기/01_OpenCV&Object_Detection/02_Object_Detection/data/coco_val/val2017.cache\n",
      "               Class     Images     Labels          P          R     mAP@.5 mAP@\n",
      "                 all       5000      36335      0.666      0.515      0.561      0.371\n",
      "Speed: 0.1ms pre-process, 1.0ms inference, 1.3ms NMS per image at shape (32, 3, 640, 640)\n",
      "\n",
      "Evaluating pycocotools mAP... saving yolov5/runs/val/exp/yolov5s_predictions.json...\n",
      "loading annotations into memory...\n",
      "pycocotools unable to run: [Errno 2] No such file or directory: '../data/coco_val/annotations/instances_val2017.json'\n",
      "Results saved to \u001b[1myolov5/runs/val/exp\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python ./yolov5/val.py --weights yolov5s.pt --data coco.yaml --img 640 --iou 0.65 --half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1e8312-812e-43f3-b9fc-03094d6e9adb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <Appendix - Calculate IoU & Recall & Precision & mAP>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97d69b-a332-47a4-9278-54466bb3df37",
   "metadata": {},
   "source": [
    "### (1) IOU(Intersection over Union) & TP(True Positive)/FP(False Positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4573dd1-3069-487c-9096-4558821219e4",
   "metadata": {},
   "source": [
    "![IoU Examples](./data/jupyter_figure/IoU.png)              ![IoU Examples](./data/jupyter_figure/IoU_example.png)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b0701b-2502-41fe-ab4a-5a7bf5842de7",
   "metadata": {},
   "source": [
    "$$IoU = {area(B_{gt} \\cap B_{pred}) \\over area(B_{gt} \\cup B_{pred})}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d025ab5-d9a0-4805-a1c6-210737de82c9",
   "metadata": {},
   "source": [
    "- Confusion Matrix by IoU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acf37f7-509f-4cc3-97ea-53fdf067bb2b",
   "metadata": {},
   "source": [
    "![IoU Examples](./data/jupyter_figure/Confusion_Matrix.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9900011-0852-4216-a5a7-d07a5a1e3d23",
   "metadata": {},
   "source": [
    "- TP(True Positive : 실제 양성 / 예측 양성 - 검출되어야 할 영역이 잘 검출되었음) : 올바른 탐지 [IoU >= Threshold]\n",
    "- FP(False Positive : 실제 음성 / 예측 양성 - 객체가 없는 영역에 검출이 되었음) : 오탐지 [IoU <= Threshold] (IoU가 임계값 보다 낮다는 것은 객체가 없는 곳에 예측 B-box를 그렸단 것이므로 FP가 된다)\n",
    "- FN(False Negative : 실제 양성 / 예측 음성 - 객체가 있는 영역에 검출이 되지 않음) : 미탐지 (Ground Truth 에는 라벨링 된 Box가 존재하나, Detector가 탐지하지 못한 경우)\n",
    "- TN(True Negative : 실제 음성 / 예측 음성 - 객체가 없는 영역에 검출이 되지 않음) : Object Detection에서 해당 지표는 사용하지 않음. 객체가 없는 영역이란 고려하지 않기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da7f8d8-c6d2-448d-8c56-fed3e3a687ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check Image Size\n",
    "org_img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f528199e-7d61-43fa-995a-ddfd76122aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Width / Height\n",
    "img_h = org_img.shape[0]\n",
    "img_w = org_img.shape[1]\n",
    "\n",
    "#Make white Image\n",
    "iou_image = np.full((img_h,img_w,3),255,np.uint8)\n",
    "\n",
    "plt.imshow(iou_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f80a169-addd-4264-bc3b-4fd63d965083",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw Predicted result B-box\n",
    "for obj_dict in obj_list : \n",
    "    cv2.rectangle(iou_image, (obj_dict['bbox'][2],obj_dict['bbox'][3]), (obj_dict['bbox'][0],obj_dict['bbox'][1]), color=pred_color[obj_dict['class']],thickness=2)\n",
    "    cnt = cnt + 1\n",
    "\n",
    "#Draw Ground Truth B-box \n",
    "for anno_dict in anno_list : \n",
    "    cv2.rectangle(iou_image, (anno_dict['bbox'][2],anno_dict['bbox'][3]), (anno_dict['bbox'][0],anno_dict['bbox'][1]), color=gt_color[anno_dict['class']],thickness=2)\n",
    "    cnt = cnt + 1\n",
    "    \n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(iou_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfcab65-8f61-423d-b2b9-a6bff53834a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Prediction classes\n",
    "pred_classes = []\n",
    "for obj_dict in obj_list :\n",
    "    if  obj_dict['class'] not in pred_classes :\n",
    "        pred_classes.append(obj_dict['class'])\n",
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48916b67-2513-4276-87ca-29cc039eb5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split obj_List by pred_classes\n",
    "class_objdict = {}\n",
    "for i in range(len(pred_classes)) :\n",
    "    class_List = []\n",
    "    for obj_dict in obj_list :\n",
    "        if pred_classes[i] == obj_dict['class'] :\n",
    "            class_List.append(obj_dict)\n",
    "    class_objdict[class_List[0]['class']] = class_List\n",
    "class_objdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffb2bb0-b119-4bac-ae44-3db5d0e2cca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorted by Confidence for all class lists\n",
    "for list_class in class_objdict.values() : \n",
    "    list_class.sort(key=lambda x:x['confidence'],reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cf946b-94a6-4216-85d3-b605fdad77ec",
   "metadata": {},
   "source": [
    "- Calculate IoU, TP/FP for Person class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5821a1-5cfd-40b5-8c3c-1db518d122ce",
   "metadata": {},
   "source": [
    "                  ![Intersection_coordinates.PNG](./data/jupyter_figure/Intersection_coordinates.PNG)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e6e21a-c196-46b5-8915-a8df08a1098d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Dataframe\n",
    "df_dict = {}\n",
    "for i in range(len(pred_classes)) : \n",
    "    df = pd.DataFrame(columns=['Detected class','confidence','IoU','TP or FP'])\n",
    "    df_dict[pred_classes[i]] = df\n",
    "df_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0130de5c-c2a9-469e-8d45-364df9212989",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find Best IoU with Ground Truth for each predicted B-box\n",
    "for key,class_objlist in class_objdict.items() : \n",
    "    for class_dict in class_objlist :\n",
    "        best_iou = 0\n",
    "        best_Anno = None\n",
    "        for anno_dict in anno_list :\n",
    "            #x1,y1 - predicted B-box\n",
    "            min_x1 = class_dict['bbox'][0]\n",
    "            min_y1 = class_dict['bbox'][1]\n",
    "            max_x1 = class_dict['bbox'][2]\n",
    "            max_y1 = class_dict['bbox'][3]\n",
    "\n",
    "            #x2,y2 - Ground Truth B-box\n",
    "            min_x2 = anno_dict['bbox'][0]\n",
    "            min_y2 = anno_dict['bbox'][1]\n",
    "            max_x2 = anno_dict['bbox'][2]\n",
    "            max_y2 = anno_dict['bbox'][3]\n",
    "\n",
    "            #TO DO - set intersection coordinate\n",
    "            interMin_x = max(min_x1,min_x2)\n",
    "            interMin_y = max(min_y1,min_y2)\n",
    "            interMax_x = min(max_x1,max_x2)\n",
    "            interMax_y = min(max_y1,max_y2)\n",
    "\n",
    "            #Calculate intersection area\n",
    "            inter_w = (interMax_x - interMin_x)\n",
    "            inter_h = (interMax_y - interMin_y)\n",
    "            if inter_w < 0 :\n",
    "                inter_w = 0\n",
    "                \n",
    "            if inter_h < 0 :\n",
    "                inter_h = 0\n",
    "                \n",
    "            intersection = inter_w * inter_h\n",
    "            #Calculate union area\n",
    "            union = ((max_x1 - min_x1) * (max_y1 - min_y1)) + ((max_x2 - min_x2) * (max_y2 - min_y2)) - intersection\n",
    "            \n",
    "            #Calculate IoU\n",
    "            iou = intersection / union\n",
    "            if best_iou < iou : \n",
    "                best_iouAnno = anno_dict #for check predict class == gt class\n",
    "                best_iou = iou\n",
    "                #print(best_iou)\n",
    "        #print(best_iou)\n",
    "        if best_iou >= 0.5 and class_dict['class'] == best_iouAnno['class'] : #check iou >= threshold & predict class == gt class\n",
    "            result = 'TP'\n",
    "        else :\n",
    "            result = 'FP'\n",
    "\n",
    "        row = [class_dict['class'],class_dict['confidence'],best_iou,result]\n",
    "        df_dict[key].loc[len(df_dict[key])] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7676423-56bf-4308-ab10-2d2286029049",
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_name in pred_classes :\n",
    "    df_dict[class_name] = df_dict[class_name].sort_values('confidence',ascending=False)\n",
    "    df_dict[class_name] = df_dict[class_name].reset_index(drop=True) #행번호 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1d1f7-29f6-44c6-93dd-a405eaa81471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "for key,values in df_dict.items() :\n",
    "    print('[' + key +']')\n",
    "    print(tabulate(values,headers='keys',tablefmt='psql',showindex=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1250b2b0-dc8f-4b70-8b91-0791df118030",
   "metadata": {},
   "source": [
    "- Calculate IoU, TP/FP for Frisbee class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ed9fce-0c9a-495c-b0a4-320552d74968",
   "metadata": {},
   "source": [
    "### (2) Accumulate TP and FP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cc8752-6940-4e43-951f-8033aca6ee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Accumulate TP / Accumulate FP Column\n",
    "for value in df_dict.values() :\n",
    "    accTP_col = [pd.NA for i in range(len(value))]\n",
    "    accFP_col = [pd.NA for i in range(len(value))]\n",
    "    value['Accumulate TP'] = accTP_col\n",
    "    value['Accumulate FP'] = accFP_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823987a3-956d-4629-ab5a-4a5715f448be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "for key,values in df_dict.items() :\n",
    "    print('[' + key +']')\n",
    "    print(tabulate(values,headers='keys',tablefmt='psql',showindex=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f156c972-095a-407e-a160-bbdc7e8b4dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate Accumulate TP or FP\n",
    "pd.set_option('mode.chained_assignment',  None) #Off Warnings\n",
    "\n",
    "for df in df_dict.values() : \n",
    "    cnt = 0\n",
    "    tporfp = df['TP or FP']\n",
    "    num_TP = 0\n",
    "    num_FP = 0\n",
    "    for tf in tporfp : \n",
    "        if tf == 'TP' :\n",
    "            if num_TP == 0 and num_FP == 0 :\n",
    "                num_TP = 1\n",
    "                num_FP = 0\n",
    "            else :\n",
    "                num_TP = df['Accumulate TP'][cnt-1] + 1\n",
    "                num_FP = df['Accumulate FP'][cnt-1]\n",
    "        else :\n",
    "            if num_TP == 0 and num_FP == 0 :\n",
    "                num_TP = 0\n",
    "                num_FP = 1\n",
    "            else :\n",
    "                num_TP = df['Accumulate TP'][cnt-1]\n",
    "                num_FP = df['Accumulate FP'][cnt-1] + 1\n",
    "        df['Accumulate TP'][cnt] = num_TP\n",
    "        df['Accumulate FP'][cnt] = num_FP\n",
    "        cnt = cnt + 1        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec593f5-0658-4472-957b-8d31a638b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key,values in df_dict.items() :\n",
    "    print('[' + key +']')\n",
    "    print(tabulate(values,headers='keys',tablefmt='psql',showindex=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da27d26-f77a-433a-bdd2-b084ae955308",
   "metadata": {},
   "source": [
    "### (3) Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b84ab1-df48-405d-846f-6a4638c90480",
   "metadata": {},
   "source": [
    "- Precision(정밀도) 는 모델 예측 값이 양성(Positive)인 대상 중 예측과 실제 값이 양성으로 일치하는 비율\n",
    "- 모델이 얼마나 정확한지를 측정하는 척도\n",
    "- 예) 모델이 객체로 검출한 수 : 10, 그 중 올바르게 검출한 갯수 : 5 => Precision = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9ab090-7ac4-4911-9413-9453e4080e49",
   "metadata": {},
   "source": [
    "$$Precision = { TP \\over TP + FP} = {TP \\over All\\,Detections}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab9b0fa-df85-4cce-8cdf-1dda2ef83001",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_Dict = {}\n",
    "for key, class_dict in df_dict.items() : \n",
    "    precision_List = []\n",
    "    for (acc_TP, acc_FP) in zip(class_dict['Accumulate TP'],class_dict['Accumulate FP']) :\n",
    "        precision = acc_TP / (acc_TP + acc_FP)\n",
    "        precision_List.append(precision)\n",
    "    precision_Dict[key] = precision_List\n",
    "precision_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79ed8ba-ec17-4561-8401-dbe9deb5c661",
   "metadata": {},
   "source": [
    "### (4) Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766de029-4b6d-4d8d-ba7d-3bba0880ce38",
   "metadata": {},
   "source": [
    "- Recall(재현율)은 실제 값이 양성(Positive)인 대상 중 예측과 실제값이 양성으로 일치하는 비율\n",
    "- 모델이 얼마나 양성(Positive) 값들을 잘 찾는지 측정하는 척도\n",
    "- 예) 검출해야하는 객체 수 : 10개, 모델이 객체로 올바르게 검출한 수 : 7 => Recall = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cb397-364d-48ed-b211-e2b210114748",
   "metadata": {},
   "source": [
    "$$Recall = {TP \\over TP + FN} = {TP \\over All\\,Ground\\,Truths}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8804d9-b69b-4b41-8265-0f387a7aa05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "#make annotation class num list\n",
    "anno_classList = []\n",
    "for anno_dict in anno_list :\n",
    "    anno_classList.append(anno_dict['class'])\n",
    "\n",
    "#Count number of annotation classes\n",
    "gt_number = collections.Counter(anno_classList)\n",
    "\n",
    "print(\"All annotation Class list : \" + str(anno_classList))\n",
    "print(\"Prediction Class Type List : \" + str(pred_classes))\n",
    "print(gt_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3faa4e-97f4-43fb-ad51-772140b08411",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add Wrong predict classes in to anno_classList \n",
    "gt_number = dict(gt_number) # Change to Dictionary\n",
    "\n",
    "#Find wrong predict classes through compare with anno_classList\n",
    "wrongpred_classList = []\n",
    "for pred_class in pred_classes :\n",
    "    if pred_class not in gt_number.keys() :\n",
    "        gt_number[pred_class] = 0\n",
    "print(gt_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da7f11-5389-4c83-aedb-a3e0f32d26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_Dict = {}\n",
    "for key, class_dict in df_dict.items() :\n",
    "    recall_List = []\n",
    "    for (acc_TP, acc_FP) in zip(class_dict['Accumulate TP'],class_dict['Accumulate FP']) :\n",
    "        if gt_number[key]!=0 : \n",
    "            recall = acc_TP / gt_number[key] #All Ground Truths(per Classes)\n",
    "            recall_List.append(recall)\n",
    "        else : #Prevent Division by Zero\n",
    "            recall = 0\n",
    "            recall_List.append(recall)\n",
    "    recall_Dict[key] = recall_List\n",
    "recall_Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834667ca-c33c-4785-90d0-701ec35f1edb",
   "metadata": {},
   "source": [
    "### (5) PR Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513906b-0d2b-4cf0-bb42-25f6ec2f2a04",
   "metadata": {},
   "source": [
    "- Precision 과 Recall 은 반비례의 관계를 가짐\n",
    "- 정확도와 검출율의 성능 변화를 확인하기 위해 X축을 Recall으로, Y 축을 Precision 으로 한 PR 곡선을 사용함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee71995a-b687-4781-b06f-08a012612af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,10))\n",
    "xlabels = ['Person PR-Curve', 'Tennis Racket PR-Curve', 'Car PR-Curve']\n",
    "rows = 2\n",
    "cols = 2\n",
    "cnt = 0\n",
    "for precision_List, recall_List in zip(precision_Dict.values(),recall_Dict.values()) :\n",
    "    ax = fig.add_subplot(rows,cols, cnt+1)\n",
    "    ax.scatter(recall_List,precision_List,color = 'red')\n",
    "    ax.plot(recall_List,precision_List,label = 'PR-Curve')\n",
    "    ax.set_xlabel(xlabels[cnt], fontsize = 15)\n",
    "    ax.set_ylim(-0.1,1.1)\n",
    "    ax.grid()\n",
    "    ax.legend()\n",
    "    cnt = cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad30fff-b5eb-4b90-a11c-d3bd5097880f",
   "metadata": {},
   "source": [
    "### (6) AP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26200402-b846-4e19-a0f8-cc0a2d087733",
   "metadata": {},
   "source": [
    "- PR 곡선을 단조적으로 감소하도록 만든 그래프의 면적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdf7350-b1ab-41e2-bbc1-8bc3a20cf40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, precision_List in precision_Dict.items() : \n",
    "    tmp = -1 \n",
    "    for i in range(len(precision_List)-1) :\n",
    "        if precision_List[i] < precision_List[i+1] :\n",
    "            precision_List[i] = precision_List[i+1]\n",
    "    precision_Dict[key] = precision_List\n",
    "precision_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276159f4-3b3b-4c33-a856-4756dd5109da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize = (20,20))\n",
    "rows = 3\n",
    "cols = 2\n",
    "cnt = 0\n",
    "for (recall_key,recall_List), (precision_key,precision_List) in zip(recall_Dict.items(), precision_Dict.items()) : \n",
    "    #Plot Monotonic Decrease PR-Curve\n",
    "    ax1 = fig.add_subplot(rows,cols,cnt+1)\n",
    "    ax1.set_title(\"Monotonic Decrease PR-Curve [\" + recall_key + \"]\" , fontsize = 20)\n",
    "    ax1.plot(recall_List,precision_List,label = 'Monotonic Decrease PR-Curve')\n",
    "    ax1.scatter(recall_List,precision_List,color = 'red')\n",
    "    ax1.set_ylim(-0.1,1.1)\n",
    "    ax1.set_xlabel(\"Recall\",fontsize=20)\n",
    "    ax1.set_ylabel(\"Precision\",fontsize=20)\n",
    "    ax1.legend()\n",
    "    ax1.grid()\n",
    "    cnt = cnt + 1\n",
    "    \n",
    "    #Plot Area of Graph\n",
    "    ax2 = fig.add_subplot(rows,cols,cnt+1)\n",
    "    ax2.set_title(\"Monotonic Decrease PR-Curve Area [\" + recall_key + \"]\" , fontsize = 20)\n",
    "    ax2.plot(recall_List,precision_List,label = 'Monotonic Decrease PR-Curve')\n",
    "    ax2.scatter(recall_List,precision_List,color = 'red')\n",
    "    ax2.fill_between(recall_List,precision_List)\n",
    "    ax2.set_xlabel(\"Recall\",fontsize=20)\n",
    "    ax2.set_ylabel(\"Precision\",fontsize=20)\n",
    "    ax2.legend()\n",
    "    cnt = cnt + 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460c2812-eae9-4cba-b2c8-e9a27a550fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate AP and make AP DICT\n",
    "ap_dict = {}\n",
    "for (precision_key, precision), (recall_key, recall) in zip(precision_Dict.items(),recall_Dict.items()) :\n",
    "    precision = torch.Tensor(precision)\n",
    "    recall = torch.Tensor(recall)\n",
    "    ap = torch.trapz(precision,recall) #Calculate AP by Integral f(x,y) / trapz(y,x) #\n",
    "    ap_dict[precision_key] = float(ap)\n",
    "\n",
    "#Print classes's AP\n",
    "for ap_class, ap in ap_dict.items() :\n",
    "    print(\"AP of \" + ap_class + \" : \" + str(ap))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4387328-664d-4e3a-b2fb-b52e857b5618",
   "metadata": {},
   "source": [
    "### (7) mAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d8f3b6-3df2-42cc-ad02-164960a96e28",
   "metadata": {},
   "source": [
    "- 각 클래스 별 AP의 평균\n",
    "- mAP가 높을 수록 검출 모델의 성능이 좋은 것으로 판단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d14b5c-4545-4c4c-bc70-4675bf160d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ap = sum(ap_dict.values())\n",
    "mAP = sum_ap/len(pred_classes)\n",
    "print(\"mAP of YOLO v5s for this image : %.2f %%\" %(mAP*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_edu",
   "language": "python",
   "name": "cv_edu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
